{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"FakeNews Detection with all features with BERT.ipynb","provenance":[{"file_id":"1W8yYgQJ6AAvwZ3AdfuPDLirqXOtHbmuO","timestamp":1575561824183},{"file_id":"https://github.com/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb","timestamp":1571609284061}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"dCpvgG0vwXAZ","colab_type":"text"},"source":["#Fake News Detection with BERT using News Content and User Engagement features"]},{"cell_type":"markdown","metadata":{"id":"xiYrZKaHwV81","colab_type":"text"},"source":["Here, we'll train a model to predict whether a news article is fake or not using features from the news articles as well as user engagement data from Twitter. Code was adapted from [this colab notebook](https://colab.research.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb)."]},{"cell_type":"code","metadata":{"id":"hsZvic2YxnTz","colab_type":"code","outputId":"f8985392-7828-4e9c-df9d-d233b8d56f8e","executionInfo":{"status":"ok","timestamp":1575839308319,"user_tz":480,"elapsed":2661,"user":{"displayName":"Divya Sudhakar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBAEe8lSbL-gO8W2QGjfXvHbFVw-wl2B28bQD3_6_A=s64","userId":"13661723492181317645"}},"colab":{"base_uri":"https://localhost:8080/","height":63}},"source":["from sklearn.model_selection import train_test_split\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","from datetime import datetime"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"cp5wfXDx5SPH","colab_type":"text"},"source":["In addition to the standard libraries we imported above, we'll need to install BERT's python package."]},{"cell_type":"code","metadata":{"id":"jviywGyWyKsA","colab_type":"code","outputId":"be7afa3d-9bd9-4a46-c585-0a4c14306588","executionInfo":{"status":"ok","timestamp":1575839312778,"user_tz":480,"elapsed":7088,"user":{"displayName":"Divya Sudhakar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBAEe8lSbL-gO8W2QGjfXvHbFVw-wl2B28bQD3_6_A=s64","userId":"13661723492181317645"}},"colab":{"base_uri":"https://localhost:8080/","height":139}},"source":["!pip install bert-tensorflow"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting bert-tensorflow\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/66/7eb4e8b6ea35b7cc54c322c816f976167a43019750279a8473d355800a93/bert_tensorflow-1.0.1-py2.py3-none-any.whl (67kB)\n","\r\u001b[K     |████▉                           | 10kB 29.8MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 20kB 6.3MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 30kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 40kB 5.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 51kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 61kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 5.7MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n","Installing collected packages: bert-tensorflow\n","Successfully installed bert-tensorflow-1.0.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hhbGEfwgdEtw","colab_type":"code","outputId":"275d6635-3b80-4dbe-8a98-97b49143e1e7","executionInfo":{"status":"ok","timestamp":1575839313169,"user_tz":480,"elapsed":7448,"user":{"displayName":"Divya Sudhakar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBAEe8lSbL-gO8W2QGjfXvHbFVw-wl2B28bQD3_6_A=s64","userId":"13661723492181317645"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["import bert\n","from bert import run_classifier\n","from bert import optimization\n","from bert import tokenization"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KVB3eOcjxxm1","colab_type":"text"},"source":["Below, we'll set an output directory location to store our model output and checkpoints. This can be a local directory, in which case you'd set OUTPUT_DIR to the name of the directory you'd like to create. If you're running this code in Google's hosted Colab, the directory won't persist after the Colab session ends.\n","\n","Alternatively, if you're a GCP user, you can store output in a GCP bucket. To do that, set a directory name in OUTPUT_DIR and the name of the GCP bucket in the BUCKET field.\n","\n","Set DO_DELETE to rewrite the OUTPUT_DIR if it exists. Otherwise, Tensorflow will load existing model checkpoints from that directory (if they exist)."]},{"cell_type":"code","metadata":{"id":"US_EAnICvP7f","colab_type":"code","outputId":"2300f75d-3480-4c81-b996-23bca43f6de8","cellView":"form","executionInfo":{"status":"ok","timestamp":1575839313176,"user_tz":480,"elapsed":2360,"user":{"displayName":"Divya Sudhakar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBAEe8lSbL-gO8W2QGjfXvHbFVw-wl2B28bQD3_6_A=s64","userId":"13661723492181317645"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Set the output directory for saving model file\n","# Optionally, set a GCP bucket location\n","\n","OUTPUT_DIR = 'model_out3'#@param {type:\"string\"}\n","#@markdown Whether or not to clear/delete the directory and create a new one\n","DO_DELETE = False #@param {type:\"boolean\"}\n","#@markdown Set USE_BUCKET and BUCKET if you want to (optionally) store model output on GCP bucket.\n","USE_BUCKET = False #@param {type:\"boolean\"}\n","BUCKET = 'BUCKET_NAME' #@param {type:\"string\"}\n","\n","if USE_BUCKET:\n","  OUTPUT_DIR = 'gs://{}/{}'.format(BUCKET, OUTPUT_DIR)\n","  from google.colab import auth\n","  auth.authenticate_user()\n","\n","if DO_DELETE:\n","  try:\n","    tf.gfile.DeleteRecursively(OUTPUT_DIR)\n","  except:\n","    # Doesn't matter if the directory didn't exist\n","    pass\n","tf.gfile.MakeDirs(OUTPUT_DIR)\n","print('***** Model output directory: {} *****'.format(OUTPUT_DIR))\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["***** Model output directory: model_out3 *****\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pmFYvkylMwXn","colab_type":"text"},"source":["#Data Loading"]},{"cell_type":"code","metadata":{"id":"mpwu5uH3zI5T","colab_type":"code","colab":{}},"source":["def strconcat(series):\n","  return series.str.cat(sep = '. ')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"n5J0LnLhzJ3A","colab_type":"code","colab":{}},"source":["def retfirst(series):\n","  return series.iloc[0]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MJq-JzW8zMLf","colab_type":"code","colab":{}},"source":["def categorize_label(series):\n","  m = series.mean()\n","  return 1 if m > 0.5 else 0 if m < 0.5 else np.nan"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fom_ff20gyy6","colab_type":"code","colab":{}},"source":["from tensorflow import keras\n","import os\n","import re\n","from google.colab import drive\n","\n","# Download and process the dataset files.\n","def download_and_load_datasets(force_download=False):\n","  drive.mount('/content/drive')\n","  unpickled_df = pd.read_pickle('/content/drive/My Drive/FakeNewsNet-master/Downloaded Datasets/fakenewsnet_dataset_tweets.pkl')\n","  # Drop rows with missing news info.\n","  unpickled_df['news_source'].replace('', np.nan, inplace=True)\n","  unpickled_df['news_text'].replace('', np.nan, inplace=True)\n","  unpickled_df = unpickled_df.dropna(subset=['news_source', 'news_text'])\n","  # Drop user PII columns.\n","  unpickled_df.drop(columns=['user_screen_name', 'user_id', 'user_location'], inplace=True)\n","  # Group by news id and aggregate the remaining columns appropriately.\n","  grouped_df = unpickled_df.groupby(['news_id']).agg({\n","      'tweet' : [strconcat], \n","      'label' : [categorize_label],\n","      'user_verified' : [categorize_label],\n","      'user_friends_count' : ['mean'],\n","      'user_followers_count' : ['mean'],\n","      'user_listed_count': ['mean'],\n","      'user_statuses_count': ['mean'],\n","      'user_favourites_count': ['mean'],\n","      'title': [retfirst],\n","      'news_source': [retfirst],\n","      'news_text': [retfirst],\n","    })\n","  # Rename columns since the grouping and aggregation results in tuples as\n","  # column names..\n","  grouped_df.columns = ['tweet', 'label', 'user_verified', 'user_friends_count',\n","                      'user_followers_count', 'user_listed_count', 'user_statuses_count',\n","                      'user_favourites_count', 'title', 'news_source', 'news_text']\n","  # Recast label as bool.\n","  grouped_df['label'] = grouped_df['label'].astype(bool)\n","  # Recast user_verified as int.\n","  grouped_df = grouped_df.dropna(subset=['user_verified'])\n","  grouped_df['user_verified'] = grouped_df['user_verified'].astype(int)\n","\n","  # Split into train/dev/test sets.\n","  grouped_df = grouped_df.sample(frac=1).reset_index(drop=True)\n","  train_df = grouped_df[0:18000]\n","  dev_df = grouped_df[18000:19000]\n","  test_df = grouped_df[19000:]\n","\n","  return train_df, dev_df, test_df\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2abfwdn-g135","colab_type":"code","outputId":"0195359d-ec42-45d4-cd8b-8e60440df001","executionInfo":{"status":"ok","timestamp":1575839352957,"user_tz":480,"elapsed":35937,"user":{"displayName":"Divya Sudhakar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBAEe8lSbL-gO8W2QGjfXvHbFVw-wl2B28bQD3_6_A=s64","userId":"13661723492181317645"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["train, dev, test = download_and_load_datasets()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"sfRnHSz3iSXz","colab_type":"text"},"source":["For us, our input data is the 'text' and 'titile' columns and our label is the 'label' column."]},{"cell_type":"code","metadata":{"id":"IuMOGwFui4it","colab_type":"code","colab":{}},"source":["# label_list is the list of labels, i.e. True, False or 0, 1 or 'dog', 'cat'\n","label_list = [True, False]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XOsZJbLGCDRY","colab_type":"text"},"source":["#Data Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"V399W0rqNJ-Z","colab_type":"text"},"source":["##Data Preprocessing for BERT\n","We'll need to transform our data into a format BERT understands. This involves two steps. First, we create  `InputExample`'s using the constructor provided in the BERT library.\n","\n","- `text_a` is the text we want to classify, which in this case, is the `Request` field in our Dataframe. \n","- `text_b` is used if we're training a model to understand the relationship between sentences (i.e. is `text_b` a translation of `text_a`? Is `text_b` an answer to the question asked by `text_a`?). This doesn't apply to our task, so we can leave `text_b` blank.\n","- `label` is the label for our example, i.e. True, False"]},{"cell_type":"code","metadata":{"id":"p9gEt5SmM6i6","colab_type":"code","colab":{}},"source":["# Use the InputExample class from BERT's run_classifier code to create examples from the data\n","train_InputExamples = train.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n","                                                                   text_a = x['title'] + x['news_text'] + x['tweet'], \n","                                                                   text_b = None, \n","                                                                   label = x['label']), axis = 1)\n","\n","dev_InputExamples = dev.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n","                                                                   text_a = x['title'] + x['news_text'] + x['tweet'], \n","                                                                   text_b = None, \n","                                                                   label = x['label']), axis = 1)\n","\n","test_InputExamples = test.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n","                                                                   text_a = x['title'] + x['news_text'] + x['tweet'], \n","                                                                   text_b = None, \n","                                                                   label = x['label']), axis = 1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SCZWZtKxObjh","colab_type":"text"},"source":["Next, we need to preprocess our data so that it matches the data BERT was trained on. For this, we'll need to do a couple of things (but don't worry--this is also included in the Python library):\n","\n","\n","1. Lowercase our text (if we're using a BERT lowercase model)\n","2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\n","3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\n","4. Map our words to indexes using a vocab file that BERT provides\n","5. Add special \"CLS\" and \"SEP\" tokens (see the [readme](https://github.com/google-research/bert))\n","6. Append \"index\" and \"segment\" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\n","\n","Happily, we don't have to worry about most of these details.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"qMWiDtpyQSoU","colab_type":"text"},"source":["To start, we'll need to load a vocabulary file and lowercasing information directly from the BERT tf hub module:"]},{"cell_type":"code","metadata":{"id":"IhJSe0QHNG7U","colab_type":"code","outputId":"afb91bf0-cfc2-4f08-bf84-2a25655e97a4","executionInfo":{"status":"ok","timestamp":1575839376168,"user_tz":480,"elapsed":10042,"user":{"displayName":"Divya Sudhakar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBAEe8lSbL-gO8W2QGjfXvHbFVw-wl2B28bQD3_6_A=s64","userId":"13661723492181317645"}},"colab":{"base_uri":"https://localhost:8080/","height":139}},"source":["# This is a path to an uncased (all lowercase) version of BERT\n","BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n","\n","def create_tokenizer_from_hub_module():\n","  \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n","  with tf.Graph().as_default():\n","    bert_module = hub.Module(BERT_MODEL_HUB)\n","    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n","    with tf.Session() as sess:\n","      vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n","                                            tokenization_info[\"do_lower_case\"]])\n","      \n","  return bert.tokenization.FullTokenizer(\n","      vocab_file=vocab_file, do_lower_case=do_lower_case)\n","\n","tokenizer = create_tokenizer_from_hub_module()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n","\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n","\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"z4oFkhpZBDKm","colab_type":"text"},"source":["Great--we just learned that the BERT model we're using expects lowercase data (that's what stored in tokenization_info[\"do_lower_case\"]) and we also loaded BERT's vocab file. We also created a tokenizer, which breaks words into word pieces:"]},{"cell_type":"code","metadata":{"id":"dsBo6RCtQmwx","colab_type":"code","outputId":"7af293d0-5aaf-4779-e1ca-f3130c2abfb1","executionInfo":{"status":"ok","timestamp":1575788684689,"user_tz":480,"elapsed":917,"user":{"displayName":"Divya Sudhakar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBAEe8lSbL-gO8W2QGjfXvHbFVw-wl2B28bQD3_6_A=s64","userId":"13661723492181317645"}},"colab":{"base_uri":"https://localhost:8080/","height":340}},"source":["tokenizer.tokenize(\"This here's an example of using the BERT tokenizer!! Here is the text.\")"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['this',\n"," 'here',\n"," \"'\",\n"," 's',\n"," 'an',\n"," 'example',\n"," 'of',\n"," 'using',\n"," 'the',\n"," 'bert',\n"," 'token',\n"," '##izer',\n"," '!',\n"," '!',\n"," 'here',\n"," 'is',\n"," 'the',\n"," 'text',\n"," '.']"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"0OEzfFIt6GIc","colab_type":"text"},"source":["Using our tokenizer, we'll call `run_classifier.convert_examples_to_features` on our InputExamples to convert them into features BERT understands."]},{"cell_type":"code","metadata":{"id":"LL5W8gEGRTAf","colab_type":"code","outputId":"cbeeda56-01c4-484b-b032-6b084de5a410","executionInfo":{"status":"ok","timestamp":1575840110512,"user_tz":480,"elapsed":734327,"user":{"displayName":"Divya Sudhakar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBAEe8lSbL-gO8W2QGjfXvHbFVw-wl2B28bQD3_6_A=s64","userId":"13661723492181317645"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# We'll set sequences to be at most 128 tokens long.\n","MAX_SEQ_LENGTH = 128\n","# Convert our train and test features to InputFeatures that BERT understands.\n","train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n","dev_features = bert.run_classifier.convert_examples_to_features(dev_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n","test_features = bert.run_classifier.convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n","\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Writing example 0 of 18000\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Writing example 0 of 18000\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] as it happened : prince harry and meg ##han mark ##le engaged - is this finally the modern bride the royal family needs ? prince harry is to marry his american actor girlfriend meg ##han mark ##le , clarence house has announced . the official announcement said : “ his royal highness the prince of wales is delighted to announce the engagement of prince harry to ms meg ##han mark ##le . the wedding will take place in spring 2018 . further details about the wedding day will be announced in due course . “ his royal highness and ms mark ##le became engaged in london earlier this month . prince harry has informed her majesty the queen and other close members of his family . [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] as it happened : prince harry and meg ##han mark ##le engaged - is this finally the modern bride the royal family needs ? prince harry is to marry his american actor girlfriend meg ##han mark ##le , clarence house has announced . the official announcement said : “ his royal highness the prince of wales is delighted to announce the engagement of prince harry to ms meg ##han mark ##le . the wedding will take place in spring 2018 . further details about the wedding day will be announced in due course . “ his royal highness and ms mark ##le became engaged in london earlier this month . prince harry has informed her majesty the queen and other close members of his family . [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 2004 2009 3047 1024 3159 4302 1998 12669 4819 2928 2571 5117 1011 2003 2023 2633 1996 2715 8959 1996 2548 2155 3791 1029 3159 4302 2003 2000 5914 2010 2137 3364 6513 12669 4819 2928 2571 1010 11805 2160 2038 2623 1012 1996 2880 8874 2056 1024 1523 2010 2548 17114 1996 3159 1997 3575 2003 15936 2000 14970 1996 8147 1997 3159 4302 2000 5796 12669 4819 2928 2571 1012 1996 5030 2097 2202 2173 1999 3500 2760 1012 2582 4751 2055 1996 5030 2154 2097 2022 2623 1999 2349 2607 1012 1523 2010 2548 17114 1998 5796 2928 2571 2150 5117 1999 2414 3041 2023 3204 1012 3159 4302 2038 6727 2014 9995 1996 3035 1998 2060 2485 2372 1997 2010 2155 1012 102\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 2004 2009 3047 1024 3159 4302 1998 12669 4819 2928 2571 5117 1011 2003 2023 2633 1996 2715 8959 1996 2548 2155 3791 1029 3159 4302 2003 2000 5914 2010 2137 3364 6513 12669 4819 2928 2571 1010 11805 2160 2038 2623 1012 1996 2880 8874 2056 1024 1523 2010 2548 17114 1996 3159 1997 3575 2003 15936 2000 14970 1996 8147 1997 3159 4302 2000 5796 12669 4819 2928 2571 1012 1996 5030 2097 2202 2173 1999 3500 2760 1012 2582 4751 2055 1996 5030 2154 2097 2022 2623 1999 2349 2607 1012 1523 2010 2548 17114 1998 5796 2928 2571 2150 5117 1999 2414 3041 2023 3204 1012 3159 4302 2038 6727 2014 9995 1996 3035 1998 2060 2485 2372 1997 2010 2155 1012 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: True (id = 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: True (id = 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] ko ##urt ##ney ka ##rda ##shi ##an asks ex scott di ##sic ##k to respect her ' boundaries ' after he enters her bedroom ##kou ##rt ##ney ka ##rda ##shi ##an is setting some boundaries with scott di ##sic ##k . although di ##sic ##k , 34 , used to live with the keeping up with the ka ##rda ##shi ##ans star and their three children — sons mason , 8 , and reign , 3 , and daughter penelope , 5 , with di ##sic ##k — ka ##rda ##sha ##in , 38 , is now putting some rules in place about what her ex can and cannot do when visiting her home . “ i just felt like we should discuss some of the [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] ko ##urt ##ney ka ##rda ##shi ##an asks ex scott di ##sic ##k to respect her ' boundaries ' after he enters her bedroom ##kou ##rt ##ney ka ##rda ##shi ##an is setting some boundaries with scott di ##sic ##k . although di ##sic ##k , 34 , used to live with the keeping up with the ka ##rda ##shi ##ans star and their three children — sons mason , 8 , and reign , 3 , and daughter penelope , 5 , with di ##sic ##k — ka ##rda ##sha ##in , 38 , is now putting some rules in place about what her ex can and cannot do when visiting her home . “ i just felt like we should discuss some of the [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 12849 19585 5420 10556 13639 6182 2319 5176 4654 3660 4487 19570 2243 2000 4847 2014 1005 7372 1005 2044 2002 8039 2014 5010 24861 5339 5420 10556 13639 6182 2319 2003 4292 2070 7372 2007 3660 4487 19570 2243 1012 2348 4487 19570 2243 1010 4090 1010 2109 2000 2444 2007 1996 4363 2039 2007 1996 10556 13639 6182 6962 2732 1998 2037 2093 2336 1517 4124 6701 1010 1022 1010 1998 5853 1010 1017 1010 1998 2684 18641 1010 1019 1010 2007 4487 19570 2243 1517 10556 13639 7377 2378 1010 4229 1010 2003 2085 5128 2070 3513 1999 2173 2055 2054 2014 4654 2064 1998 3685 2079 2043 5873 2014 2188 1012 1523 1045 2074 2371 2066 2057 2323 6848 2070 1997 1996 102\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 12849 19585 5420 10556 13639 6182 2319 5176 4654 3660 4487 19570 2243 2000 4847 2014 1005 7372 1005 2044 2002 8039 2014 5010 24861 5339 5420 10556 13639 6182 2319 2003 4292 2070 7372 2007 3660 4487 19570 2243 1012 2348 4487 19570 2243 1010 4090 1010 2109 2000 2444 2007 1996 4363 2039 2007 1996 10556 13639 6182 6962 2732 1998 2037 2093 2336 1517 4124 6701 1010 1022 1010 1998 5853 1010 1017 1010 1998 2684 18641 1010 1019 1010 2007 4487 19570 2243 1517 10556 13639 7377 2378 1010 4229 1010 2003 2085 5128 2070 3513 1999 2173 2055 2054 2014 4654 2064 1998 3685 2079 2043 5873 2014 2188 1012 1523 1045 2074 2371 2066 2057 2323 6848 2070 1997 1996 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: False (id = 1)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: False (id = 1)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] mel ##ania trump and kate middleton are style twins - but who is the real queen of fashion ? kate middleton , 35 , is known for dressing fabulous ##ly for special state occasions . the mother - of - two is also one for following a colour scheme . this has notably been the case for st patrick ’ s day each year , when kate wears a variation on green in homage to the emerald isle . in 17 march , 2012 the british royal , who wed prince william in 2011 , wore a long , dark green coat and she also wore green to attend the st patrick ' s day parade at mons barracks on march 17 , 2014 in al [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] mel ##ania trump and kate middleton are style twins - but who is the real queen of fashion ? kate middleton , 35 , is known for dressing fabulous ##ly for special state occasions . the mother - of - two is also one for following a colour scheme . this has notably been the case for st patrick ’ s day each year , when kate wears a variation on green in homage to the emerald isle . in 17 march , 2012 the british royal , who wed prince william in 2011 , wore a long , dark green coat and she also wore green to attend the st patrick ' s day parade at mons barracks on march 17 , 2014 in al [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 11463 13241 8398 1998 5736 17756 2024 2806 8178 1011 2021 2040 2003 1996 2613 3035 1997 4827 1029 5736 17756 1010 3486 1010 2003 2124 2005 11225 18783 2135 2005 2569 2110 6642 1012 1996 2388 1011 1997 1011 2048 2003 2036 2028 2005 2206 1037 6120 5679 1012 2023 2038 5546 2042 1996 2553 2005 2358 4754 1521 1055 2154 2169 2095 1010 2043 5736 11651 1037 8386 2006 2665 1999 14822 2000 1996 14110 8842 1012 1999 2459 2233 1010 2262 1996 2329 2548 1010 2040 21981 3159 2520 1999 2249 1010 5078 1037 2146 1010 2601 2665 5435 1998 2016 2036 5078 2665 2000 5463 1996 2358 4754 1005 1055 2154 7700 2012 29408 10492 2006 2233 2459 1010 2297 1999 2632 102\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 11463 13241 8398 1998 5736 17756 2024 2806 8178 1011 2021 2040 2003 1996 2613 3035 1997 4827 1029 5736 17756 1010 3486 1010 2003 2124 2005 11225 18783 2135 2005 2569 2110 6642 1012 1996 2388 1011 1997 1011 2048 2003 2036 2028 2005 2206 1037 6120 5679 1012 2023 2038 5546 2042 1996 2553 2005 2358 4754 1521 1055 2154 2169 2095 1010 2043 5736 11651 1037 8386 2006 2665 1999 14822 2000 1996 14110 8842 1012 1999 2459 2233 1010 2262 1996 2329 2548 1010 2040 21981 3159 2520 1999 2249 1010 5078 1037 2146 1010 2601 2665 5435 1998 2016 2036 5078 2665 2000 5463 1996 2358 4754 1005 1055 2154 7700 2012 29408 10492 2006 2233 2459 1010 2297 1999 2632 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: False (id = 1)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: False (id = 1)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] dj k ##hale ##d ' s baby son steals the show in gu ##cci at mtv v ##mas ##he ' s already well - accustomed to working the red carpet , despite being only 10 months old . but dj k ##hale ##d ' s adorable baby son asa ##hd well and truly stole the show at the mtv video music awards at the forum in ing ##lewood , california , on sunday night . the to ##t looked incredible as he sport ##ed a custom - made gu ##cci suit to match his record producer father ' s look . cut ##ie : dj k ##hale ##d ' s adorable baby son asa ##hd , 10 months , well and truly stole the show at [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] dj k ##hale ##d ' s baby son steals the show in gu ##cci at mtv v ##mas ##he ' s already well - accustomed to working the red carpet , despite being only 10 months old . but dj k ##hale ##d ' s adorable baby son asa ##hd well and truly stole the show at the mtv video music awards at the forum in ing ##lewood , california , on sunday night . the to ##t looked incredible as he sport ##ed a custom - made gu ##cci suit to match his record producer father ' s look . cut ##ie : dj k ##hale ##d ' s adorable baby son asa ##hd , 10 months , well and truly stole the show at [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 6520 1047 15238 2094 1005 1055 3336 2365 15539 1996 2265 1999 19739 14693 2012 8692 1058 9335 5369 1005 1055 2525 2092 1011 17730 2000 2551 1996 2417 10135 1010 2750 2108 2069 2184 2706 2214 1012 2021 6520 1047 15238 2094 1005 1055 23677 3336 2365 17306 14945 2092 1998 5621 10312 1996 2265 2012 1996 8692 2678 2189 2982 2012 1996 7057 1999 13749 26580 1010 2662 1010 2006 4465 2305 1012 1996 2000 2102 2246 9788 2004 2002 4368 2098 1037 7661 1011 2081 19739 14693 4848 2000 2674 2010 2501 3135 2269 1005 1055 2298 1012 3013 2666 1024 6520 1047 15238 2094 1005 1055 23677 3336 2365 17306 14945 1010 2184 2706 1010 2092 1998 5621 10312 1996 2265 2012 102\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 6520 1047 15238 2094 1005 1055 3336 2365 15539 1996 2265 1999 19739 14693 2012 8692 1058 9335 5369 1005 1055 2525 2092 1011 17730 2000 2551 1996 2417 10135 1010 2750 2108 2069 2184 2706 2214 1012 2021 6520 1047 15238 2094 1005 1055 23677 3336 2365 17306 14945 2092 1998 5621 10312 1996 2265 2012 1996 8692 2678 2189 2982 2012 1996 7057 1999 13749 26580 1010 2662 1010 2006 4465 2305 1012 1996 2000 2102 2246 9788 2004 2002 4368 2098 1037 7661 1011 2081 19739 14693 4848 2000 2674 2010 2501 3135 2269 1005 1055 2298 1012 3013 2666 1024 6520 1047 15238 2094 1005 1055 23677 3336 2365 17306 14945 1010 2184 2706 1010 2092 1998 5621 10312 1996 2265 2012 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: False (id = 1)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: False (id = 1)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] chance the rapper : 5 things you didn ’ t know about 2017 bet humanitarian award winner ##chan ##ce the rapper : 5 things you didn ’ t know about 2017 bet humanitarian award winner chance the rapper is taking the music industry by storm . in the first week of its may 2016 release , the 24 - year - old chicago native ’ s third mixtape , coloring book , was streamed a … http : / / if ##t . tt / 2 ##s ##6 ##ii ##dm ##chan ##ce the rapper : 5 things you didn ##t know about 2017 bet humanitarian award winner https : / / t . co / d ##5 ##k ##z ##q ##rc ##v ##f ##d https : [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] chance the rapper : 5 things you didn ’ t know about 2017 bet humanitarian award winner ##chan ##ce the rapper : 5 things you didn ’ t know about 2017 bet humanitarian award winner chance the rapper is taking the music industry by storm . in the first week of its may 2016 release , the 24 - year - old chicago native ’ s third mixtape , coloring book , was streamed a … http : / / if ##t . tt / 2 ##s ##6 ##ii ##dm ##chan ##ce the rapper : 5 things you didn ##t know about 2017 bet humanitarian award winner https : / / t . co / d ##5 ##k ##z ##q ##rc ##v ##f ##d https : [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 3382 1996 10687 1024 1019 2477 2017 2134 1521 1056 2113 2055 2418 6655 11470 2400 3453 14856 3401 1996 10687 1024 1019 2477 2017 2134 1521 1056 2113 2055 2418 6655 11470 2400 3453 3382 1996 10687 2003 2635 1996 2189 3068 2011 4040 1012 1999 1996 2034 2733 1997 2049 2089 2355 2713 1010 1996 2484 1011 2095 1011 2214 3190 3128 1521 1055 2353 18713 1010 22276 2338 1010 2001 18498 1037 1529 8299 1024 1013 1013 2065 2102 1012 23746 1013 1016 2015 2575 6137 22117 14856 3401 1996 10687 1024 1019 2477 2017 2134 2102 2113 2055 2418 6655 11470 2400 3453 16770 1024 1013 1013 1056 1012 2522 1013 1040 2629 2243 2480 4160 11890 2615 2546 2094 16770 1024 102\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 3382 1996 10687 1024 1019 2477 2017 2134 1521 1056 2113 2055 2418 6655 11470 2400 3453 14856 3401 1996 10687 1024 1019 2477 2017 2134 1521 1056 2113 2055 2418 6655 11470 2400 3453 3382 1996 10687 2003 2635 1996 2189 3068 2011 4040 1012 1999 1996 2034 2733 1997 2049 2089 2355 2713 1010 1996 2484 1011 2095 1011 2214 3190 3128 1521 1055 2353 18713 1010 22276 2338 1010 2001 18498 1037 1529 8299 1024 1013 1013 2065 2102 1012 23746 1013 1016 2015 2575 6137 22117 14856 3401 1996 10687 1024 1019 2477 2017 2134 2102 2113 2055 2418 6655 11470 2400 3453 16770 1024 1013 1013 1056 1012 2522 1013 1040 2629 2243 2480 4160 11890 2615 2546 2094 16770 1024 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: False (id = 1)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: False (id = 1)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Writing example 10000 of 18000\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Writing example 10000 of 18000\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Writing example 0 of 1000\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Writing example 0 of 1000\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] jessica bi ##el shares sweet birthday message to ' super - hot dad ' justin timber ##lake ##just ##in timber ##lake is getting spoiled with love ! the \" filthy \" singer turned 37 on wednesday and his wife , jessica bi ##el , couldn ' t help but gus ##h about her \" super - hot \" hub ##by on his birthday . taking to ins ##tagram , the actress shared a sweet picture of the two embracing each other . \" a picture says a thousand words . and thank goodness because there aren ' t enough to express all the aspects of my love and respect for you , \" bi ##el began her note . \" here ' s to a spectacular [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] jessica bi ##el shares sweet birthday message to ' super - hot dad ' justin timber ##lake ##just ##in timber ##lake is getting spoiled with love ! the \" filthy \" singer turned 37 on wednesday and his wife , jessica bi ##el , couldn ' t help but gus ##h about her \" super - hot \" hub ##by on his birthday . taking to ins ##tagram , the actress shared a sweet picture of the two embracing each other . \" a picture says a thousand words . and thank goodness because there aren ' t enough to express all the aspects of my love and respect for you , \" bi ##el began her note . \" here ' s to a spectacular [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 8201 12170 2884 6661 4086 5798 4471 2000 1005 3565 1011 2980 3611 1005 6796 7227 14530 29427 2378 7227 14530 2003 2893 19582 2007 2293 999 1996 1000 18294 1000 3220 2357 4261 2006 9317 1998 2010 2564 1010 8201 12170 2884 1010 2481 1005 1056 2393 2021 12670 2232 2055 2014 1000 3565 1011 2980 1000 9594 3762 2006 2010 5798 1012 2635 2000 16021 23091 1010 1996 3883 4207 1037 4086 3861 1997 1996 2048 23581 2169 2060 1012 1000 1037 3861 2758 1037 4595 2616 1012 1998 4067 15003 2138 2045 4995 1005 1056 2438 2000 4671 2035 1996 5919 1997 2026 2293 1998 4847 2005 2017 1010 1000 12170 2884 2211 2014 3602 1012 1000 2182 1005 1055 2000 1037 12656 102\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 8201 12170 2884 6661 4086 5798 4471 2000 1005 3565 1011 2980 3611 1005 6796 7227 14530 29427 2378 7227 14530 2003 2893 19582 2007 2293 999 1996 1000 18294 1000 3220 2357 4261 2006 9317 1998 2010 2564 1010 8201 12170 2884 1010 2481 1005 1056 2393 2021 12670 2232 2055 2014 1000 3565 1011 2980 1000 9594 3762 2006 2010 5798 1012 2635 2000 16021 23091 1010 1996 3883 4207 1037 4086 3861 1997 1996 2048 23581 2169 2060 1012 1000 1037 3861 2758 1037 4595 2616 1012 1998 4067 15003 2138 2045 4995 1005 1056 2438 2000 4671 2035 1996 5919 1997 2026 2293 1998 4847 2005 2017 1010 1000 12170 2884 2211 2014 3602 1012 1000 2182 1005 1055 2000 1037 12656 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: False (id = 1)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: False (id = 1)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] sean penn says he and ex robin wright don ' t ' share the same ethical views on parenting ' sean penn is opening up about his relationship with his ex - wife , robin wright . the duo , who divorced in 2010 and share two children — dylan , 26 , and hopper , 24 — “ don ’ t have a lot of conversation , ” penn told comedian marc mar ##on on his w ##tf podcast . “ we don ’ t not get along , ” he added . “ we have very separate relationships with our kids at this point and it seems to work better that way because they are making their own decisions . as it turned out [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] sean penn says he and ex robin wright don ' t ' share the same ethical views on parenting ' sean penn is opening up about his relationship with his ex - wife , robin wright . the duo , who divorced in 2010 and share two children — dylan , 26 , and hopper , 24 — “ don ’ t have a lot of conversation , ” penn told comedian marc mar ##on on his w ##tf podcast . “ we don ’ t not get along , ” he added . “ we have very separate relationships with our kids at this point and it seems to work better that way because they are making their own decisions . as it turned out [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 5977 9502 2758 2002 1998 4654 5863 6119 2123 1005 1056 1005 3745 1996 2168 12962 5328 2006 28586 1005 5977 9502 2003 3098 2039 2055 2010 3276 2007 2010 4654 1011 2564 1010 5863 6119 1012 1996 6829 1010 2040 9196 1999 2230 1998 3745 2048 2336 1517 7758 1010 2656 1010 1998 20517 1010 2484 1517 1523 2123 1521 1056 2031 1037 2843 1997 4512 1010 1524 9502 2409 9971 7871 9388 2239 2006 2010 1059 24475 16110 1012 1523 2057 2123 1521 1056 2025 2131 2247 1010 1524 2002 2794 1012 1523 2057 2031 2200 3584 6550 2007 2256 4268 2012 2023 2391 1998 2009 3849 2000 2147 2488 2008 2126 2138 2027 2024 2437 2037 2219 6567 1012 2004 2009 2357 2041 102\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 5977 9502 2758 2002 1998 4654 5863 6119 2123 1005 1056 1005 3745 1996 2168 12962 5328 2006 28586 1005 5977 9502 2003 3098 2039 2055 2010 3276 2007 2010 4654 1011 2564 1010 5863 6119 1012 1996 6829 1010 2040 9196 1999 2230 1998 3745 2048 2336 1517 7758 1010 2656 1010 1998 20517 1010 2484 1517 1523 2123 1521 1056 2031 1037 2843 1997 4512 1010 1524 9502 2409 9971 7871 9388 2239 2006 2010 1059 24475 16110 1012 1523 2057 2123 1521 1056 2025 2131 2247 1010 1524 2002 2794 1012 1523 2057 2031 2200 3584 6550 2007 2256 4268 2012 2023 2391 1998 2009 3849 2000 2147 2488 2008 2126 2138 2027 2024 2437 2037 2219 6567 1012 2004 2009 2357 2041 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: False (id = 1)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: False (id = 1)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] laguna beach al ##um taylor cole welcome ##s first child , daughter tatum ##ima ##ge zoom taylor cole taylor cole / ins ##tagram taylor cole ‘ s baby is here ! the laguna beach season 2 star has welcomed her first baby , a daughter with husband kevin sims ##haus ##er . “ our baby girl , tatum woods is here , ” cole announced on her ins ##tagram story friday , along with photos of the new dad cr ##ad ##ling their baby girl . “ sm ##itte ##n with my daddy , ” the mother of one capt ##ioned a father - daughter photo . in addition , cole shared images of family members holding the newborn . image zoom taylor cole taylor cole [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] laguna beach al ##um taylor cole welcome ##s first child , daughter tatum ##ima ##ge zoom taylor cole taylor cole / ins ##tagram taylor cole ‘ s baby is here ! the laguna beach season 2 star has welcomed her first baby , a daughter with husband kevin sims ##haus ##er . “ our baby girl , tatum woods is here , ” cole announced on her ins ##tagram story friday , along with photos of the new dad cr ##ad ##ling their baby girl . “ sm ##itte ##n with my daddy , ” the mother of one capt ##ioned a father - daughter photo . in addition , cole shared images of family members holding the newborn . image zoom taylor cole taylor cole [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 18169 3509 2632 2819 4202 5624 6160 2015 2034 2775 1010 2684 25913 9581 3351 24095 4202 5624 4202 5624 1013 16021 23091 4202 5624 1520 1055 3336 2003 2182 999 1996 18169 3509 2161 1016 2732 2038 10979 2014 2034 3336 1010 1037 2684 2007 3129 4901 18135 13821 2121 1012 1523 2256 3336 2611 1010 25913 5249 2003 2182 1010 1524 5624 2623 2006 2014 16021 23091 2466 5958 1010 2247 2007 7760 1997 1996 2047 3611 13675 4215 2989 2037 3336 2611 1012 1523 15488 27100 2078 2007 2026 8600 1010 1524 1996 2388 1997 2028 14408 19798 1037 2269 1011 2684 6302 1012 1999 2804 1010 5624 4207 4871 1997 2155 2372 3173 1996 20662 1012 3746 24095 4202 5624 4202 5624 102\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 18169 3509 2632 2819 4202 5624 6160 2015 2034 2775 1010 2684 25913 9581 3351 24095 4202 5624 4202 5624 1013 16021 23091 4202 5624 1520 1055 3336 2003 2182 999 1996 18169 3509 2161 1016 2732 2038 10979 2014 2034 3336 1010 1037 2684 2007 3129 4901 18135 13821 2121 1012 1523 2256 3336 2611 1010 25913 5249 2003 2182 1010 1524 5624 2623 2006 2014 16021 23091 2466 5958 1010 2247 2007 7760 1997 1996 2047 3611 13675 4215 2989 2037 3336 2611 1012 1523 15488 27100 2078 2007 2026 8600 1010 1524 1996 2388 1997 2028 14408 19798 1037 2269 1011 2684 6302 1012 1999 2804 1010 5624 4207 4871 1997 2155 2372 3173 1996 20662 1012 3746 24095 4202 5624 4202 5624 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: False (id = 1)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: False (id = 1)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] the worst hit song of every year since 2000 , from mack ##lem ##ore to train ( twice ) over time , music genres shift in popularity , songwriting methods change , but one thing remains constant in popular music each year — terrible songs find their way to the top of the charts . billboard com ##pile ##s an annual list of the top 100 singles at year ' s end . we revisited the most popular songs of every year since 2000 and selected what we deemed to be the worst song from each year ' s list of top 10 singles . if you feel that times have changed for the worse , consider that we ' re at least no longer in [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] the worst hit song of every year since 2000 , from mack ##lem ##ore to train ( twice ) over time , music genres shift in popularity , songwriting methods change , but one thing remains constant in popular music each year — terrible songs find their way to the top of the charts . billboard com ##pile ##s an annual list of the top 100 singles at year ' s end . we revisited the most popular songs of every year since 2000 and selected what we deemed to be the worst song from each year ' s list of top 10 singles . if you feel that times have changed for the worse , consider that we ' re at least no longer in [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 1996 5409 2718 2299 1997 2296 2095 2144 2456 1010 2013 11349 16930 5686 2000 3345 1006 3807 1007 2058 2051 1010 2189 11541 5670 1999 6217 1010 14029 4725 2689 1010 2021 2028 2518 3464 5377 1999 2759 2189 2169 2095 1517 6659 2774 2424 2037 2126 2000 1996 2327 1997 1996 6093 1012 4908 4012 22090 2015 2019 3296 2862 1997 1996 2327 2531 3895 2012 2095 1005 1055 2203 1012 2057 24354 1996 2087 2759 2774 1997 2296 2095 2144 2456 1998 3479 2054 2057 8357 2000 2022 1996 5409 2299 2013 2169 2095 1005 1055 2862 1997 2327 2184 3895 1012 2065 2017 2514 2008 2335 2031 2904 2005 1996 4788 1010 5136 2008 2057 1005 2128 2012 2560 2053 2936 1999 102\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 1996 5409 2718 2299 1997 2296 2095 2144 2456 1010 2013 11349 16930 5686 2000 3345 1006 3807 1007 2058 2051 1010 2189 11541 5670 1999 6217 1010 14029 4725 2689 1010 2021 2028 2518 3464 5377 1999 2759 2189 2169 2095 1517 6659 2774 2424 2037 2126 2000 1996 2327 1997 1996 6093 1012 4908 4012 22090 2015 2019 3296 2862 1997 1996 2327 2531 3895 2012 2095 1005 1055 2203 1012 2057 24354 1996 2087 2759 2774 1997 2296 2095 2144 2456 1998 3479 2054 2057 8357 2000 2022 1996 5409 2299 2013 2169 2095 1005 1055 2862 1997 2327 2184 3895 1012 2065 2017 2514 2008 2335 2031 2904 2005 1996 4788 1010 5136 2008 2057 1005 2128 2012 2560 2053 2936 1999 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: True (id = 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: True (id = 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] destiny ' s child star michelle williams reveals she is seeking professional help for depression ##get the biggest ce ##le ##bs stories by email sub ##scribe thank you for sub ##sc ##ri ##bing we have more newsletter ##s show me see our privacy notice could not sub ##scribe , try again later invalid email michelle williams has revealed she ' s seeking professional help for depression . the destiny ' s child singer , who has been open about suffering from depression in the past , released a statement on ins ##tagram on tuesday . she wrote : \" for years i have dedicated myself to increasing awareness of mental health and em ##powering people to recognise when it ' s time to seek help , [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] destiny ' s child star michelle williams reveals she is seeking professional help for depression ##get the biggest ce ##le ##bs stories by email sub ##scribe thank you for sub ##sc ##ri ##bing we have more newsletter ##s show me see our privacy notice could not sub ##scribe , try again later invalid email michelle williams has revealed she ' s seeking professional help for depression . the destiny ' s child singer , who has been open about suffering from depression in the past , released a statement on ins ##tagram on tuesday . she wrote : \" for years i have dedicated myself to increasing awareness of mental health and em ##powering people to recognise when it ' s time to seek help , [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 10461 1005 1055 2775 2732 9393 3766 7657 2016 2003 6224 2658 2393 2005 6245 18150 1996 5221 8292 2571 5910 3441 2011 10373 4942 29234 4067 2017 2005 4942 11020 3089 10472 2057 2031 2062 17178 2015 2265 2033 2156 2256 9394 5060 2071 2025 4942 29234 1010 3046 2153 2101 19528 10373 9393 3766 2038 3936 2016 1005 1055 6224 2658 2393 2005 6245 1012 1996 10461 1005 1055 2775 3220 1010 2040 2038 2042 2330 2055 6114 2013 6245 1999 1996 2627 1010 2207 1037 4861 2006 16021 23091 2006 9857 1012 2016 2626 1024 1000 2005 2086 1045 2031 4056 2870 2000 4852 7073 1997 5177 2740 1998 7861 23948 2111 2000 17614 2043 2009 1005 1055 2051 2000 6148 2393 1010 102\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 10461 1005 1055 2775 2732 9393 3766 7657 2016 2003 6224 2658 2393 2005 6245 18150 1996 5221 8292 2571 5910 3441 2011 10373 4942 29234 4067 2017 2005 4942 11020 3089 10472 2057 2031 2062 17178 2015 2265 2033 2156 2256 9394 5060 2071 2025 4942 29234 1010 3046 2153 2101 19528 10373 9393 3766 2038 3936 2016 1005 1055 6224 2658 2393 2005 6245 1012 1996 10461 1005 1055 2775 3220 1010 2040 2038 2042 2330 2055 6114 2013 6245 1999 1996 2627 1010 2207 1037 4861 2006 16021 23091 2006 9857 1012 2016 2626 1024 1000 2005 2086 1045 2031 4056 2870 2000 4852 7073 1997 5177 2740 1998 7861 23948 2111 2000 17614 2043 2009 1005 1055 2051 2000 6148 2393 1010 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: False (id = 1)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: False (id = 1)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Writing example 0 of 1118\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Writing example 0 of 1118\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] taylor swift ’ s ‘ tonight show ’ performance was so emotional , it ’ ll have you sobbing ##ta ##yl ##or swift performed on the tonight show starring jimmy fallon on monday , nov . 13 , and the video is emotional on its own . but one of the show ' s employees revealed some more details about her performance through a twitter thread last night , and it will make you sob . last night was fallon ' s first night back on the show following the death of his mother , gloria fallon , on nov . 4 . fallon paid an emotional tribute to his mother during the show , during which he revealed some private things about his relationship with [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] taylor swift ’ s ‘ tonight show ’ performance was so emotional , it ’ ll have you sobbing ##ta ##yl ##or swift performed on the tonight show starring jimmy fallon on monday , nov . 13 , and the video is emotional on its own . but one of the show ' s employees revealed some more details about her performance through a twitter thread last night , and it will make you sob . last night was fallon ' s first night back on the show following the death of his mother , gloria fallon , on nov . 4 . fallon paid an emotional tribute to his mother during the show , during which he revealed some private things about his relationship with [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 4202 9170 1521 1055 1520 3892 2265 1521 2836 2001 2061 6832 1010 2009 1521 2222 2031 2017 20040 2696 8516 2953 9170 2864 2006 1996 3892 2265 4626 5261 16443 2006 6928 1010 13292 1012 2410 1010 1998 1996 2678 2003 6832 2006 2049 2219 1012 2021 2028 1997 1996 2265 1005 1055 5126 3936 2070 2062 4751 2055 2014 2836 2083 1037 10474 11689 2197 2305 1010 1998 2009 2097 2191 2017 17540 1012 2197 2305 2001 16443 1005 1055 2034 2305 2067 2006 1996 2265 2206 1996 2331 1997 2010 2388 1010 10778 16443 1010 2006 13292 1012 1018 1012 16443 3825 2019 6832 7050 2000 2010 2388 2076 1996 2265 1010 2076 2029 2002 3936 2070 2797 2477 2055 2010 3276 2007 102\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 4202 9170 1521 1055 1520 3892 2265 1521 2836 2001 2061 6832 1010 2009 1521 2222 2031 2017 20040 2696 8516 2953 9170 2864 2006 1996 3892 2265 4626 5261 16443 2006 6928 1010 13292 1012 2410 1010 1998 1996 2678 2003 6832 2006 2049 2219 1012 2021 2028 1997 1996 2265 1005 1055 5126 3936 2070 2062 4751 2055 2014 2836 2083 1037 10474 11689 2197 2305 1010 1998 2009 2097 2191 2017 17540 1012 2197 2305 2001 16443 1005 1055 2034 2305 2067 2006 1996 2265 2206 1996 2331 1997 2010 2388 1010 10778 16443 1010 2006 13292 1012 1018 1012 16443 3825 2019 6832 7050 2000 2010 2388 2076 1996 2265 1010 2076 2029 2002 3936 2070 2797 2477 2055 2010 3276 2007 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: False (id = 1)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: False (id = 1)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] international arrest warrant issued for george so ##ros ##ge ##org ##e so ##ros — the billionaire investment banker who has admitted to manipulating the financial markets in asia , the uk , greece , and russia has finally gone too far . you see mr . so ##ros has become persona non gr ##ata across the globe for his role in des ##ta ##bil ##izing country ’ s economy ’ s and financial markets . he does so for the sole intent of lining his own pockets at the expense of others . george so ##ros now lives in the united states and has been involved in many of the anti - trump protests around the country . he has paid salaries and housing for many [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] international arrest warrant issued for george so ##ros ##ge ##org ##e so ##ros — the billionaire investment banker who has admitted to manipulating the financial markets in asia , the uk , greece , and russia has finally gone too far . you see mr . so ##ros has become persona non gr ##ata across the globe for his role in des ##ta ##bil ##izing country ’ s economy ’ s and financial markets . he does so for the sole intent of lining his own pockets at the expense of others . george so ##ros now lives in the united states and has been involved in many of the anti - trump protests around the country . he has paid salaries and housing for many [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 2248 6545 10943 3843 2005 2577 2061 7352 3351 21759 2063 2061 7352 1517 1996 22301 5211 13448 2040 2038 4914 2000 26242 1996 3361 6089 1999 4021 1010 1996 2866 1010 5483 1010 1998 3607 2038 2633 2908 2205 2521 1012 2017 2156 2720 1012 2061 7352 2038 2468 16115 2512 24665 6790 2408 1996 7595 2005 2010 2535 1999 4078 2696 14454 6026 2406 1521 1055 4610 1521 1055 1998 3361 6089 1012 2002 2515 2061 2005 1996 7082 7848 1997 14834 2010 2219 10306 2012 1996 10961 1997 2500 1012 2577 2061 7352 2085 3268 1999 1996 2142 2163 1998 2038 2042 2920 1999 2116 1997 1996 3424 1011 8398 8090 2105 1996 2406 1012 2002 2038 3825 20566 1998 3847 2005 2116 102\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 2248 6545 10943 3843 2005 2577 2061 7352 3351 21759 2063 2061 7352 1517 1996 22301 5211 13448 2040 2038 4914 2000 26242 1996 3361 6089 1999 4021 1010 1996 2866 1010 5483 1010 1998 3607 2038 2633 2908 2205 2521 1012 2017 2156 2720 1012 2061 7352 2038 2468 16115 2512 24665 6790 2408 1996 7595 2005 2010 2535 1999 4078 2696 14454 6026 2406 1521 1055 4610 1521 1055 1998 3361 6089 1012 2002 2515 2061 2005 1996 7082 7848 1997 14834 2010 2219 10306 2012 1996 10961 1997 2500 1012 2577 2061 7352 2085 3268 1999 1996 2142 2163 1998 2038 2042 2920 1999 2116 1997 1996 3424 1011 8398 8090 2105 1996 2406 1012 2002 2038 3825 20566 1998 3847 2005 2116 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: True (id = 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: True (id = 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] blake & gwen quit ##ting hollywood & moving to midwest after mis ##carriage nightmare ##ac ##cor ##ding to radar online , blake shelton and gwen stefan ##i are calling it quit ##s … on hollywood . the couple reportedly suffered a mis ##carriage and gwen is devastated . although blake is okay with not having kids of his own , the mis ##carriage has broken gwen so much that she has quit fertility treatments and looking for sur ##rogate ##s . for now the wedding plans are also reportedly on hold while the couple relocate ##s to be closer to blake ’ s family in oklahoma . have you made a move from the big city to a small town ? blake shelton and gwen stefan [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] blake & gwen quit ##ting hollywood & moving to midwest after mis ##carriage nightmare ##ac ##cor ##ding to radar online , blake shelton and gwen stefan ##i are calling it quit ##s … on hollywood . the couple reportedly suffered a mis ##carriage and gwen is devastated . although blake is okay with not having kids of his own , the mis ##carriage has broken gwen so much that she has quit fertility treatments and looking for sur ##rogate ##s . for now the wedding plans are also reportedly on hold while the couple relocate ##s to be closer to blake ’ s family in oklahoma . have you made a move from the big city to a small town ? blake shelton and gwen stefan [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 6511 1004 11697 8046 3436 5365 1004 3048 2000 13608 2044 28616 21539 10103 6305 27108 4667 2000 7217 3784 1010 6511 22230 1998 11697 8852 2072 2024 4214 2009 8046 2015 1529 2006 5365 1012 1996 3232 7283 4265 1037 28616 21539 1998 11697 2003 13879 1012 2348 6511 2003 3100 2007 2025 2383 4268 1997 2010 2219 1010 1996 28616 21539 2038 3714 11697 2061 2172 2008 2016 2038 8046 17376 13441 1998 2559 2005 7505 21799 2015 1012 2005 2085 1996 5030 3488 2024 2036 7283 2006 2907 2096 1996 3232 20102 2015 2000 2022 3553 2000 6511 1521 1055 2155 1999 5858 1012 2031 2017 2081 1037 2693 2013 1996 2502 2103 2000 1037 2235 2237 1029 6511 22230 1998 11697 8852 102\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 6511 1004 11697 8046 3436 5365 1004 3048 2000 13608 2044 28616 21539 10103 6305 27108 4667 2000 7217 3784 1010 6511 22230 1998 11697 8852 2072 2024 4214 2009 8046 2015 1529 2006 5365 1012 1996 3232 7283 4265 1037 28616 21539 1998 11697 2003 13879 1012 2348 6511 2003 3100 2007 2025 2383 4268 1997 2010 2219 1010 1996 28616 21539 2038 3714 11697 2061 2172 2008 2016 2038 8046 17376 13441 1998 2559 2005 7505 21799 2015 1012 2005 2085 1996 5030 3488 2024 2036 7283 2006 2907 2096 1996 3232 20102 2015 2000 2022 3553 2000 6511 1521 1055 2155 1999 5858 1012 2031 2017 2081 1037 2693 2013 1996 2502 2103 2000 1037 2235 2237 1029 6511 22230 1998 11697 8852 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: True (id = 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: True (id = 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] ko ##urt ##ney ka ##rda ##shi ##an cl ##ari ##fies she is not pregnant : ' that was completely taken out of context ' despite what you may have heard , ko ##urt ##ney ka ##rda ##shi ##an is not with child . the reality star took to twitter on tuesday to di ##sp ##el several reports that she is pregnant , based on an interview she gave to graz ##ia magazine . \" the interview ##er asked me ' what ’ s a crazy rumor that you ’ ve heard about yourself ? ' and my answer was . . . ' i ’ ve heard that i am pregnant , ' \" the 38 - year - old mom of three t ##wee ##ted [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] ko ##urt ##ney ka ##rda ##shi ##an cl ##ari ##fies she is not pregnant : ' that was completely taken out of context ' despite what you may have heard , ko ##urt ##ney ka ##rda ##shi ##an is not with child . the reality star took to twitter on tuesday to di ##sp ##el several reports that she is pregnant , based on an interview she gave to graz ##ia magazine . \" the interview ##er asked me ' what ’ s a crazy rumor that you ’ ve heard about yourself ? ' and my answer was . . . ' i ’ ve heard that i am pregnant , ' \" the 38 - year - old mom of three t ##wee ##ted [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 12849 19585 5420 10556 13639 6182 2319 18856 8486 14213 2016 2003 2025 6875 1024 1005 2008 2001 3294 2579 2041 1997 6123 1005 2750 2054 2017 2089 2031 2657 1010 12849 19585 5420 10556 13639 6182 2319 2003 2025 2007 2775 1012 1996 4507 2732 2165 2000 10474 2006 9857 2000 4487 13102 2884 2195 4311 2008 2016 2003 6875 1010 2241 2006 2019 4357 2016 2435 2000 26918 2401 2932 1012 1000 1996 4357 2121 2356 2033 1005 2054 1521 1055 1037 4689 19075 2008 2017 1521 2310 2657 2055 4426 1029 1005 1998 2026 3437 2001 1012 1012 1012 1005 1045 1521 2310 2657 2008 1045 2572 6875 1010 1005 1000 1996 4229 1011 2095 1011 2214 3566 1997 2093 1056 28394 3064 102\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 12849 19585 5420 10556 13639 6182 2319 18856 8486 14213 2016 2003 2025 6875 1024 1005 2008 2001 3294 2579 2041 1997 6123 1005 2750 2054 2017 2089 2031 2657 1010 12849 19585 5420 10556 13639 6182 2319 2003 2025 2007 2775 1012 1996 4507 2732 2165 2000 10474 2006 9857 2000 4487 13102 2884 2195 4311 2008 2016 2003 6875 1010 2241 2006 2019 4357 2016 2435 2000 26918 2401 2932 1012 1000 1996 4357 2121 2356 2033 1005 2054 1521 1055 1037 4689 19075 2008 2017 1521 2310 2657 2055 4426 1029 1005 1998 2026 3437 2001 1012 1012 1012 1005 1045 1521 2310 2657 2008 1045 2572 6875 1010 1005 1000 1996 4229 1011 2095 1011 2214 3566 1997 2093 1056 28394 3064 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: True (id = 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: True (id = 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] eric decker denies cheating on wife jessie james decker : ' check ' your ' facts ' eric decker won ’ t stand for anyone questioning his fidelity . on wednesday , the football player shut down rumors that he cheated on his wife jessie james decker on twitter . responding to a since - deleted t ##wee ##t accusing the 31 - year - old of in ##fide ##lity , eric wrote , “ sorry lady the only club i ’ ve been jam ##min is the mickey mouse club house ! ( that ’ s a kids tv show for the non - parents ) . ” “ make sure your ‘ friends ’ check their facts , ” he wrote . according to [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] eric decker denies cheating on wife jessie james decker : ' check ' your ' facts ' eric decker won ’ t stand for anyone questioning his fidelity . on wednesday , the football player shut down rumors that he cheated on his wife jessie james decker on twitter . responding to a since - deleted t ##wee ##t accusing the 31 - year - old of in ##fide ##lity , eric wrote , “ sorry lady the only club i ’ ve been jam ##min is the mickey mouse club house ! ( that ’ s a kids tv show for the non - parents ) . ” “ make sure your ‘ friends ’ check their facts , ” he wrote . according to [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 4388 20946 23439 16789 2006 2564 10934 2508 20946 1024 1005 4638 1005 2115 1005 8866 1005 4388 20946 2180 1521 1056 3233 2005 3087 11242 2010 22625 1012 2006 9317 1010 1996 2374 2447 3844 2091 11256 2008 2002 22673 2006 2010 2564 10934 2508 20946 2006 10474 1012 14120 2000 1037 2144 1011 17159 1056 28394 2102 16723 1996 2861 1011 2095 1011 2214 1997 1999 20740 18605 1010 4388 2626 1010 1523 3374 3203 1996 2069 2252 1045 1521 2310 2042 9389 10020 2003 1996 11021 8000 2252 2160 999 1006 2008 1521 1055 1037 4268 2694 2265 2005 1996 2512 1011 3008 1007 1012 1524 1523 2191 2469 2115 1520 2814 1521 4638 2037 8866 1010 1524 2002 2626 1012 2429 2000 102\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 4388 20946 23439 16789 2006 2564 10934 2508 20946 1024 1005 4638 1005 2115 1005 8866 1005 4388 20946 2180 1521 1056 3233 2005 3087 11242 2010 22625 1012 2006 9317 1010 1996 2374 2447 3844 2091 11256 2008 2002 22673 2006 2010 2564 10934 2508 20946 2006 10474 1012 14120 2000 1037 2144 1011 17159 1056 28394 2102 16723 1996 2861 1011 2095 1011 2214 1997 1999 20740 18605 1010 4388 2626 1010 1523 3374 3203 1996 2069 2252 1045 1521 2310 2042 9389 10020 2003 1996 11021 8000 2252 2160 999 1006 2008 1521 1055 1037 4268 2694 2265 2005 1996 2512 1011 3008 1007 1012 1524 1523 2191 2469 2115 1520 2814 1521 4638 2037 8866 1010 1524 2002 2626 1012 2429 2000 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: False (id = 1)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: False (id = 1)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"kS_4O6X9CIa2","colab_type":"text"},"source":["##Creating an Input Generator"]},{"cell_type":"markdown","metadata":{"id":"79BZfOltCtG8","colab_type":"text"},"source":["Next we create an input builder function that takes our training feature set (`train_features`) and produces a generator. This is a pretty standard design pattern for working with Tensorflow [Estimators](https://www.tensorflow.org/guide/estimators)."]},{"cell_type":"code","metadata":{"id":"mtESIG2OzirZ","colab_type":"code","colab":{}},"source":["def input_fn_builder(dataframe, features, seq_length, is_training, drop_remainder):\n","\n","  all_input_ids = []\n","  all_input_mask = []\n","  all_segment_ids = []\n","  all_label_ids = []\n","\n","  news_source = []\n","  user_verified = []\n","  user_friends_count = []\n","  user_followers_count = []\n","  user_listed_count = []\n","  user_statuses_count = []\n","  user_favourites_count = []\n","\n","  # Feature gen preserves the order so we can iterate over dataframe and\n","  # feature together.\n","  for i, feature in enumerate(features):\n","    all_input_ids.append(feature.input_ids)\n","    all_input_mask.append(feature.input_mask)\n","    all_segment_ids.append(feature.segment_ids)\n","    all_label_ids.append(feature.label_id)\n","\n","    row = dataframe.iloc[i]\n","    news_source.append(row.news_source)\n","    user_verified.append(row.user_verified)\n","    user_friends_count.append(row.user_friends_count)\n","    user_followers_count.append(row.user_followers_count)\n","    user_listed_count.append(row.user_listed_count)\n","    user_statuses_count.append(row.user_statuses_count)\n","    user_favourites_count.append(row.user_favourites_count)\n","\n","  news_source_categorized, _ = pd.factorize(news_source)\n","  news_source_categorized = news_source_categorized.tolist()\n","\n","  def input_fn(params):\n","    \"\"\"The actual input function.\"\"\"\n","    batch_size = params[\"batch_size\"]\n","\n","    num_examples = len(features)\n","\n","    d = tf.data.Dataset.from_tensor_slices({\n","        \"input_ids\":\n","            tf.constant(\n","                all_input_ids, shape=[num_examples, seq_length],\n","                dtype=tf.int32),\n","        \"input_mask\":\n","            tf.constant(\n","                all_input_mask,\n","                shape=[num_examples, seq_length],\n","                dtype=tf.int32),\n","        \"segment_ids\":\n","            tf.constant(\n","                all_segment_ids,\n","                shape=[num_examples, seq_length],\n","                dtype=tf.int32),\n","\n","        \"news_source\":\n","            tf.constant(news_source_categorized, shape=[num_examples], dtype=tf.float32),\n","        \"user_verified\":\n","            tf.constant(user_verified, shape=[num_examples], dtype=tf.float32),\n","        \"user_friends_count\":\n","            tf.constant(user_friends_count, shape=[num_examples], dtype=tf.float32),\n","        \"user_followers_count\":\n","            tf.constant(user_followers_count, shape=[num_examples], dtype=tf.float32),\n","        \"user_listed_count\":\n","            tf.constant(user_listed_count, shape=[num_examples], dtype=tf.float32),\n","        \"user_statuses_count\":\n","            tf.constant(user_statuses_count, shape=[num_examples], dtype=tf.float32),\n","        \"user_favourites_count\":\n","            tf.constant(user_favourites_count, shape=[num_examples], dtype=tf.float32),\n","        \"label_ids\":\n","            tf.constant(all_label_ids, shape=[num_examples], dtype=tf.int32),\n","    })\n","\n","    if is_training:\n","      d = d.repeat()\n","      d = d.shuffle(buffer_size=100)\n","\n","    d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder)\n","    return d\n","\n","  return input_fn    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1Pv2bAlOX_-K","colab_type":"code","colab":{}},"source":["# Create an input function for training. drop_remainder = True for using TPUs.\n","train_input_fn = input_fn_builder(\n","    dataframe=train,\n","    features=train_features,\n","    seq_length=MAX_SEQ_LENGTH,\n","    is_training=True,\n","    drop_remainder=False)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ccp5trMwRtmr","colab_type":"text"},"source":["#Creating a model\n","\n","Now that we've prepared our data, let's focus on building a model. `create_model` does just this below. First, it loads the BERT tf hub module again (this time to extract the computation graph). Next, it creates a single new layer that will be trained to adapt BERT to our sentiment task (i.e. classifying whether a movie review is positive or negative). This strategy of using a mostly trained model is called [fine-tuning](http://wiki.fast.ai/index.php/Fine_tuning)."]},{"cell_type":"code","metadata":{"id":"OjwJ4bTeWXD8","colab_type":"code","colab":{}},"source":["# Compute train and warmup steps from batch size\n","BATCH_SIZE = 32\n","LEARNING_RATE = 1e-3\n","NUM_TRAIN_EPOCHS = 10.0\n","# Warmup is a period of time where the learning rate \n","# is small and gradually increases--usually helps training.\n","WARMUP_PROPORTION = 0.1\n","# Model configs\n","SAVE_CHECKPOINTS_STEPS = 500\n","SAVE_SUMMARY_STEPS = 100\n","DROPOUT_PROB = 0.8"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6o2a5ZIvRcJq","colab_type":"code","colab":{}},"source":["def create_model(is_predicting, input_ids, input_mask, segment_ids,\n","                 news_source, user_verified, user_followers_count,\n","                 user_listed_count, user_statuses_count, user_favourites_count,\n","                 labels, num_labels):\n","  \"\"\"Creates a classification model.\"\"\"\n","\n","  bert_module = hub.Module(\n","      BERT_MODEL_HUB,\n","      trainable=True)\n","  bert_inputs = dict(\n","      input_ids=input_ids,\n","      input_mask=input_mask,\n","      segment_ids=segment_ids)\n","  bert_outputs = bert_module(\n","      inputs=bert_inputs,\n","      signature=\"tokens\",\n","      as_dict=True)\n","\n","\n","  # Use \"pooled_output\" for classification tasks on an entire sentence.\n","  # Use \"sequence_outputs\" for token-level output.\n","  output_layer = bert_outputs[\"pooled_output\"]\n","  \n","  hidden_size = output_layer.shape[-1].value  \n","\n","  # Create our own layer to tune for politeness data.\n","  output_weights = tf.get_variable(\n","      \"output_weights\", [num_labels, hidden_size],\n","      initializer=tf.truncated_normal_initializer(stddev=0.02))\n","  output_bias = tf.get_variable(\n","      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n","  \n","  # Create 7 extra layers for the output from BERT and the other non-BERT\n","  # inputs. We use the same number of hidden units as in the last BERT layer\n","  # for now. But this architecture hyperparameter can be tuned.\n","  layer1_weights = tf.get_variable(\n","      \"layer1_weights\", [hidden_size, 7], dtype=tf.float32,\n","      initializer=tf.truncated_normal_initializer(stddev=0.02))\n","  layer1_bias = tf.get_variable(\n","      \"layer1_bias\", [hidden_size], dtype=tf.float32, initializer=tf.zeros_initializer())\n","  \n","  layer2_weights = tf.get_variable(\n","      \"layer2_weights\", [hidden_size, hidden_size], dtype=tf.float32,\n","      initializer=tf.truncated_normal_initializer(stddev=0.02))\n","  layer2_bias = tf.get_variable(\n","      \"layer2_bias\", [hidden_size], dtype=tf.float32, initializer=tf.zeros_initializer())\n","  \n","  layer3_weights = tf.get_variable(\n","      \"layer3_weights\", [hidden_size, hidden_size], dtype=tf.float32,\n","      initializer=tf.truncated_normal_initializer(stddev=0.02))\n","  layer3_bias = tf.get_variable(\n","      \"layer3_bias\", [hidden_size], dtype=tf.float32, initializer=tf.zeros_initializer())\n","  \n","  layer4_weights = tf.get_variable(\n","      \"layer4_weights\", [hidden_size, hidden_size], dtype=tf.float32,\n","      initializer=tf.truncated_normal_initializer(stddev=0.02))\n","  layer4_bias = tf.get_variable(\n","      \"layer4_bias\", [hidden_size], dtype=tf.float32, initializer=tf.zeros_initializer())\n","  \n","  layer5_weights = tf.get_variable(\n","      \"layer5_weights\", [hidden_size, hidden_size], dtype=tf.float32,\n","      initializer=tf.truncated_normal_initializer(stddev=0.02))\n","  layer5_bias = tf.get_variable(\n","      \"layer5_bias\", [hidden_size], dtype=tf.float32, initializer=tf.zeros_initializer())\n","  \n","  layer6_weights = tf.get_variable(\n","      \"layer6_weights\", [hidden_size, hidden_size], dtype=tf.float32,\n","      initializer=tf.truncated_normal_initializer(stddev=0.02))\n","  layer6_bias = tf.get_variable(\n","      \"layer6_bias\", [hidden_size], dtype=tf.float32, initializer=tf.zeros_initializer())\n","  \n","  final_output_weights = tf.get_variable(\n","      \"final_output_weights\", [num_labels, hidden_size], dtype=tf.float32,\n","      initializer=tf.truncated_normal_initializer(stddev=0.02))\n","  final_output_bias = tf.get_variable(\n","      \"final_output_bias\", [num_labels], dtype=tf.float32, initializer=tf.zeros_initializer())\n","  \n","  with tf.variable_scope(\"loss\"):\n","    # Dropout helps prevent overfitting\n","    output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n","\n","    bert_logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n","    bert_logits = tf.nn.bias_add(bert_logits, output_bias)\n","    bert_log_probs = tf.nn.log_softmax(bert_logits, axis=-1)\n","\n","    # Use low level api.\n","    bert_prediction = tf.squeeze(tf.argmax(bert_log_probs, axis=-1, output_type=tf.int32))\n","    bert_prediction = tf.cast(bert_prediction, tf.float32)\n","    dnn_inputs = tf.stack(\n","      [news_source, user_verified, user_followers_count, user_listed_count,\n","       user_statuses_count, user_favourites_count, bert_prediction], 0)\n","    \n","    dnn_layer1 = tf.matmul(dnn_inputs, layer1_weights, transpose_a=True, transpose_b=True)\n","    dnn_layer1 = tf.nn.bias_add(dnn_layer1, layer1_bias)\n","    dnn_layer1 = tf.nn.leaky_relu(dnn_layer1, alpha=0.2, name=\"dnn_layer_1\")\n","    dnn_layer1 = tf.nn.dropout(dnn_layer1, keep_prob=DROPOUT_PROB)\n","  \n","    dnn_layer2 = tf.matmul(dnn_layer1, layer2_weights, transpose_b=True)\n","    dnn_layer2 = tf.nn.bias_add(dnn_layer2, layer2_bias)\n","    dnn_layer2 = tf.nn.leaky_relu(dnn_layer2, alpha=0.2, name=\"dnn_layer_2\")\n","    dnn_layer2 = tf.nn.dropout(dnn_layer2, keep_prob=DROPOUT_PROB)\n","\n","    dnn_layer3 = tf.matmul(dnn_layer2, layer3_weights, transpose_b=True)\n","    dnn_layer3 = tf.nn.bias_add(dnn_layer3, layer3_bias)\n","    dnn_layer3 = tf.nn.leaky_relu(dnn_layer3, alpha=0.2, name=\"dnn_layer_3\")\n","    dnn_layer3 = tf.nn.dropout(dnn_layer3, keep_prob=DROPOUT_PROB)\n","\n","    dnn_layer4 = tf.matmul(dnn_layer3, layer4_weights, transpose_b=True)\n","    dnn_layer4 = tf.nn.bias_add(dnn_layer4, layer4_bias)\n","    dnn_layer4 = tf.nn.leaky_relu(dnn_layer4, alpha=0.2, name=\"dnn_layer_4\")\n","    dnn_layer4 = tf.nn.dropout(dnn_layer4, keep_prob=DROPOUT_PROB)\n","\n","    dnn_layer5 = tf.matmul(dnn_layer4, layer5_weights, transpose_b=True)\n","    dnn_layer5 = tf.nn.bias_add(dnn_layer5, layer5_bias)\n","    dnn_layer5 = tf.nn.leaky_relu(dnn_layer5, alpha=0.2, name=\"dnn_layer_5\")\n","    dnn_layer5 = tf.nn.dropout(dnn_layer5, keep_prob=DROPOUT_PROB)\n","\n","    dnn_layer6 = tf.matmul(dnn_layer5, layer6_weights, transpose_b=True)\n","    dnn_layer6 = tf.nn.bias_add(dnn_layer6, layer6_bias)\n","    dnn_layer6 = tf.nn.leaky_relu(dnn_layer6, alpha=0.2, name=\"dnn_layer_6\")\n","    dnn_layer6 = tf.nn.dropout(dnn_layer6, keep_prob=DROPOUT_PROB)\n","\n","    # Final softmax layer\n","    logits = tf.matmul(dnn_layer6, final_output_weights, transpose_b=True)\n","    logits = tf.nn.bias_add(logits, final_output_bias)\n","    log_probs = tf.nn.log_softmax(logits, axis=-1)\n","\n","    # Convert labels into one-hot encoding\n","    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n","\n","    predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n","    # If we're predicting, we want predicted labels and the probabiltiies.\n","    if is_predicting:\n","      return (predicted_labels, log_probs)\n","\n","    # If we're train/eval, compute loss between predicted and actual label\n","    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n","    loss = tf.reduce_mean(per_example_loss)\n","    return (loss, predicted_labels, log_probs)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qpE0ZIDOCQzE","colab_type":"text"},"source":["Next we'll wrap our model function in a `model_fn_builder` function that adapts our model to work for training, evaluation, and prediction."]},{"cell_type":"code","metadata":{"id":"FnH-AnOQ9KKW","colab_type":"code","colab":{}},"source":["# model_fn_builder actually creates our model function\n","# using the passed parameters for num_labels, etc.\n","def model_fn_builder(learning_rate, num_labels, num_train_steps,\n","                     num_warmup_steps):\n","  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n","  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n","    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n","\n","    input_ids = features[\"input_ids\"]\n","    input_mask = features[\"input_mask\"]\n","    segment_ids = features[\"segment_ids\"]\n","    label_ids = features[\"label_ids\"]\n","\n","    # Features that don't go into BERT but go into the DNN layers.\n","    news_source = features[\"news_source\"]\n","    user_verified = features[\"user_verified\"]\n","    user_followers_count = features[\"user_followers_count\"]\n","    user_listed_count = features[\"user_listed_count\"]\n","    user_statuses_count = features[\"user_statuses_count\"]\n","    user_favourites_count = features[\"user_favourites_count\"]\n","\n","    is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n","    \n","    # TRAIN and EVAL\n","    if not is_predicting:\n","\n","      (loss, predicted_labels, log_probs) = create_model(\n","        is_predicting, input_ids, input_mask, segment_ids, \n","        news_source, user_verified, user_followers_count, user_listed_count,\n","        user_statuses_count, user_favourites_count, label_ids, num_labels)\n","\n","      train_op = bert.optimization.create_optimizer(\n","          loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n","\n","      # Calculate evaluation metrics. \n","      def metric_fn(label_ids, predicted_labels):\n","        accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n","        f1_score = tf.contrib.metrics.f1_score(\n","            label_ids,\n","            predicted_labels)\n","        auc = tf.metrics.auc(\n","            label_ids,\n","            predicted_labels)\n","        recall = tf.metrics.recall(\n","            label_ids,\n","            predicted_labels)\n","        precision = tf.metrics.precision(\n","            label_ids,\n","            predicted_labels) \n","        true_pos = tf.metrics.true_positives(\n","            label_ids,\n","            predicted_labels)\n","        true_neg = tf.metrics.true_negatives(\n","            label_ids,\n","            predicted_labels)   \n","        false_pos = tf.metrics.false_positives(\n","            label_ids,\n","            predicted_labels)  \n","        false_neg = tf.metrics.false_negatives(\n","            label_ids,\n","            predicted_labels)\n","        return {\n","            \"eval_accuracy\": accuracy,\n","            \"f1_score\": f1_score,\n","            \"auc\": auc,\n","            \"precision\": precision,\n","            \"recall\": recall,\n","            \"true_positives\": true_pos,\n","            \"true_negatives\": true_neg,\n","            \"false_positives\": false_pos,\n","            \"false_negatives\": false_neg,\n","        }\n","\n","      eval_metrics = metric_fn(label_ids, predicted_labels)\n","\n","      if mode == tf.estimator.ModeKeys.TRAIN:\n","        return tf.estimator.EstimatorSpec(mode=mode,\n","          loss=loss,\n","          train_op=train_op)\n","      else:\n","          return tf.estimator.EstimatorSpec(mode=mode,\n","            loss=loss,\n","            eval_metric_ops=eval_metrics)\n","    else:\n","      (predicted_labels, log_probs) = create_model(\n","        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n","\n","      predictions = {\n","          'probabilities': log_probs,\n","          'labels': predicted_labels\n","      }\n","      return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n","\n","  # Return the actual model function in the closure\n","  return model_fn\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"emHf9GhfWBZ_","colab_type":"code","colab":{}},"source":["# Compute # train and warmup steps from batch size\n","num_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n","num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oEJldMr3WYZa","colab_type":"code","colab":{}},"source":["# Specify output directory and number of checkpoint steps to save\n","run_config = tf.estimator.RunConfig(\n","    model_dir=OUTPUT_DIR,\n","    save_summary_steps=SAVE_SUMMARY_STEPS,\n","    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"q_WebpS1X97v","colab_type":"code","outputId":"d1d011b0-b7a4-497b-dc60-332ccc348766","executionInfo":{"status":"ok","timestamp":1575841531717,"user_tz":480,"elapsed":886,"user":{"displayName":"Divya Sudhakar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBAEe8lSbL-gO8W2QGjfXvHbFVw-wl2B28bQD3_6_A=s64","userId":"13661723492181317645"}},"colab":{"base_uri":"https://localhost:8080/","height":275}},"source":["model_fn = model_fn_builder(\n","  learning_rate=LEARNING_RATE,\n","  num_labels=len(label_list),\n","  num_train_steps=num_train_steps,\n","  num_warmup_steps=num_warmup_steps)\n","\n","estimator = tf.estimator.Estimator(\n","  model_fn=model_fn,\n","  config=run_config,\n","  params={\"batch_size\": BATCH_SIZE})\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Using config: {'_model_dir': 'model_out3', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n","graph_options {\n","  rewrite_options {\n","    meta_optimizer_iterations: ONE\n","  }\n","}\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f7909836b00>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Using config: {'_model_dir': 'model_out3', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n","graph_options {\n","  rewrite_options {\n","    meta_optimizer_iterations: ONE\n","  }\n","}\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f7909836b00>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"t6Nukby2EB6-","colab_type":"text"},"source":["Now we train our model! "]},{"cell_type":"code","metadata":{"id":"nucD4gluYJmK","colab_type":"code","outputId":"2720a778-2035-4ff0-93ab-717e8138ea1f","executionInfo":{"status":"ok","timestamp":1575842477934,"user_tz":480,"elapsed":933837,"user":{"displayName":"Divya Sudhakar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBAEe8lSbL-gO8W2QGjfXvHbFVw-wl2B28bQD3_6_A=s64","userId":"13661723492181317645"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["print(f'Beginning Training!')\n","current_time = datetime.now()\n","estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n","print(\"Training took time \", datetime.now() - current_time)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Beginning Training!\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Calling model_fn.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Calling model_fn.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-16-9911322270ae>:80: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-16-9911322270ae>:80: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:27: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n","\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:27: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:32: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n","\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:32: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:70: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n","\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:70: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/clip_ops.py:301: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/clip_ops.py:301: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/metrics/python/metrics/classification.py:162: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Deprecated in favor of operator or tf.math.divide.\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/metrics/python/metrics/classification.py:162: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Deprecated in favor of operator or tf.math.divide.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Done calling model_fn.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Done calling model_fn.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Create CheckpointSaverHook.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Create CheckpointSaverHook.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Graph was finalized.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Graph was finalized.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Running local_init_op.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Running local_init_op.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Done running local_init_op.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Done running local_init_op.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saving checkpoints for 0 into model_out3/model.ckpt.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Saving checkpoints for 0 into model_out3/model.ckpt.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 10.293423, step = 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 10.293423, step = 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 5.73559\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 5.73559\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 1.1746547, step = 100 (17.437 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 1.1746547, step = 100 (17.437 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.39905\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.39905\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 1.0403241, step = 200 (15.627 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 1.0403241, step = 200 (15.627 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.38961\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.38961\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.65883166, step = 301 (15.654 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.65883166, step = 301 (15.654 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.39009\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.39009\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 1.1018696, step = 400 (15.650 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 1.1018696, step = 400 (15.650 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saving checkpoints for 500 into model_out3/model.ckpt.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Saving checkpoints for 500 into model_out3/model.ckpt.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 5.69897\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 5.69897\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.6096079, step = 500 (17.550 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.6096079, step = 500 (17.550 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.38924\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.38924\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 1.0591232, step = 600 (15.646 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 1.0591232, step = 600 (15.646 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.3968\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.3968\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.3535789, step = 700 (15.631 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.3535789, step = 700 (15.631 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.40054\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.40054\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 2.3092375, step = 800 (15.624 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 2.3092375, step = 800 (15.624 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.40328\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.40328\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 1.0837166, step = 900 (15.618 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 1.0837166, step = 900 (15.618 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saving checkpoints for 1000 into model_out3/model.ckpt.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Saving checkpoints for 1000 into model_out3/model.ckpt.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 5.53983\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 5.53983\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.6306973, step = 1000 (18.054 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.6306973, step = 1000 (18.054 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.40016\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.40016\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 1.374339, step = 1101 (15.621 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 1.374339, step = 1101 (15.621 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.39833\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.39833\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.53115594, step = 1201 (15.629 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.53115594, step = 1201 (15.629 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.39325\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.39325\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.56409043, step = 1301 (15.644 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.56409043, step = 1301 (15.644 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.39164\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.39164\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.511323, step = 1400 (15.644 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.511323, step = 1400 (15.644 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saving checkpoints for 1500 into model_out3/model.ckpt.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Saving checkpoints for 1500 into model_out3/model.ckpt.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 5.69054\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 5.69054\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 1.4368689, step = 1501 (17.572 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 1.4368689, step = 1501 (17.572 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.38957\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.38957\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 1.0584142, step = 1600 (15.656 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 1.0584142, step = 1600 (15.656 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.39324\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.39324\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 1.0073246, step = 1700 (15.640 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 1.0073246, step = 1700 (15.640 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.38696\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.38696\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.83200544, step = 1801 (15.658 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.83200544, step = 1801 (15.658 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.39447\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.39447\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.76577544, step = 1900 (15.634 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.76577544, step = 1900 (15.634 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saving checkpoints for 2000 into model_out3/model.ckpt.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Saving checkpoints for 2000 into model_out3/model.ckpt.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 5.69937\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 5.69937\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.4692219, step = 2000 (17.551 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.4692219, step = 2000 (17.551 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.394\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.394\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.24653615, step = 2100 (15.635 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.24653615, step = 2100 (15.635 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.38109\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.38109\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.4049315, step = 2200 (15.675 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.4049315, step = 2200 (15.675 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.3886\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.3886\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.67879325, step = 2300 (15.650 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.67879325, step = 2300 (15.650 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.39546\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.39546\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.37943697, step = 2401 (15.636 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.37943697, step = 2401 (15.636 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saving checkpoints for 2500 into model_out3/model.ckpt.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Saving checkpoints for 2500 into model_out3/model.ckpt.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to delete files with this prefix.\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to delete files with this prefix.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 5.67957\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 5.67957\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.4689778, step = 2500 (17.607 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.4689778, step = 2500 (17.607 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.38562\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.38562\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.31174147, step = 2600 (15.660 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.31174147, step = 2600 (15.660 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.3985\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.3985\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.29119426, step = 2700 (15.629 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.29119426, step = 2700 (15.629 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.39023\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.39023\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.27970204, step = 2800 (15.649 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.27970204, step = 2800 (15.649 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.381\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.381\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.30115446, step = 2901 (15.671 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.30115446, step = 2901 (15.671 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saving checkpoints for 3000 into model_out3/model.ckpt.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Saving checkpoints for 3000 into model_out3/model.ckpt.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 5.59944\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 5.59944\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.70067066, step = 3000 (17.859 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.70067066, step = 3000 (17.859 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.39241\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.39241\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.6560622, step = 3100 (15.644 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.6560622, step = 3100 (15.644 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.39348\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.39348\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.35826388, step = 3200 (15.641 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.35826388, step = 3200 (15.641 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.39274\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.39274\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.48479098, step = 3300 (15.643 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.48479098, step = 3300 (15.643 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.39484\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.39484\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.4969565, step = 3400 (15.638 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.4969565, step = 3400 (15.638 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saving checkpoints for 3500 into model_out3/model.ckpt.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Saving checkpoints for 3500 into model_out3/model.ckpt.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 5.66307\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 5.66307\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.37639925, step = 3500 (17.658 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.37639925, step = 3500 (17.658 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.39148\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.39148\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.8133905, step = 3601 (15.646 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.8133905, step = 3601 (15.646 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.3941\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.3941\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.3129369, step = 3700 (15.640 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.3129369, step = 3700 (15.640 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.38677\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.38677\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.43476915, step = 3800 (15.660 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.43476915, step = 3800 (15.660 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.3796\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.3796\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.6005055, step = 3901 (15.673 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.6005055, step = 3901 (15.673 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saving checkpoints for 4000 into model_out3/model.ckpt.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Saving checkpoints for 4000 into model_out3/model.ckpt.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 5.66714\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 5.66714\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.38092482, step = 4000 (17.646 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.38092482, step = 4000 (17.646 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.39118\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.39118\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.44156116, step = 4100 (15.645 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.44156116, step = 4100 (15.645 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.3867\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.3867\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.29847884, step = 4200 (15.659 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.29847884, step = 4200 (15.659 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.40026\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.40026\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.4544802, step = 4301 (15.623 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.4544802, step = 4301 (15.623 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.38154\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.38154\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.4156258, step = 4400 (15.673 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.4156258, step = 4400 (15.673 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saving checkpoints for 4500 into model_out3/model.ckpt.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Saving checkpoints for 4500 into model_out3/model.ckpt.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 5.67868\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 5.67868\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.32597053, step = 4500 (17.607 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.32597053, step = 4500 (17.607 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.3928\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.3928\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.23394372, step = 4600 (15.642 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.23394372, step = 4600 (15.642 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.38765\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.38765\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.7987431, step = 4701 (15.659 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.7987431, step = 4701 (15.659 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.39132\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.39132\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.6293217, step = 4800 (15.643 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.6293217, step = 4800 (15.643 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.38552\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.38552\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.55343276, step = 4901 (15.668 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.55343276, step = 4901 (15.668 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saving checkpoints for 5000 into model_out3/model.ckpt.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Saving checkpoints for 5000 into model_out3/model.ckpt.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 5.68078\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 5.68078\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.22584733, step = 5000 (17.596 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.22584733, step = 5000 (17.596 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.39104\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.39104\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.5476562, step = 5101 (15.647 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.5476562, step = 5101 (15.647 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.38358\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.38358\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.24756612, step = 5200 (15.669 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.24756612, step = 5200 (15.669 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.38931\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.38931\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.28785655, step = 5301 (15.647 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.28785655, step = 5301 (15.647 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.38244\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.38244\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.5024116, step = 5400 (15.668 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.5024116, step = 5400 (15.668 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saving checkpoints for 5500 into model_out3/model.ckpt.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Saving checkpoints for 5500 into model_out3/model.ckpt.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 5.67852\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 5.67852\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.44141948, step = 5500 (17.610 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.44141948, step = 5500 (17.610 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.39563\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 6.39563\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.30458623, step = 5600 (15.638 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.30458623, step = 5600 (15.638 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saving checkpoints for 5625 into model_out3/model.ckpt.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Saving checkpoints for 5625 into model_out3/model.ckpt.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Loss for final step: 0.22800115.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Loss for final step: 0.22800115.\n"],"name":"stderr"},{"output_type":"stream","text":["Training took time  0:15:32.887869\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7K6Vj61NDNwh","colab_type":"text"},"source":["#Evaluate on the Dev set"]},{"cell_type":"markdown","metadata":{"id":"CmbLTVniARy3","colab_type":"text"},"source":["Now let's use our dev data to see how well our model did:"]},{"cell_type":"code","metadata":{"id":"JIhejfpyJ8Bx","colab_type":"code","colab":{}},"source":["dev_input_fn = input_fn_builder(\n","    dataframe=dev,\n","    features=dev_features,\n","    seq_length=MAX_SEQ_LENGTH,\n","    is_training=False,\n","    drop_remainder=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PPVEXhNjYXC-","colab_type":"code","outputId":"27989939-e06c-4cd8-c790-634f884550de","executionInfo":{"status":"ok","timestamp":1575842498429,"user_tz":480,"elapsed":13515,"user":{"displayName":"Divya Sudhakar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBAEe8lSbL-gO8W2QGjfXvHbFVw-wl2B28bQD3_6_A=s64","userId":"13661723492181317645"}},"colab":{"base_uri":"https://localhost:8080/","height":598}},"source":["estimator.evaluate(input_fn=dev_input_fn, steps=None)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Calling model_fn.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Calling model_fn.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Done calling model_fn.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Done calling model_fn.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Starting evaluation at 2019-12-08T22:01:28Z\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Starting evaluation at 2019-12-08T22:01:28Z\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Graph was finalized.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Graph was finalized.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from model_out3/model.ckpt-5625\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from model_out3/model.ckpt-5625\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Running local_init_op.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Running local_init_op.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Done running local_init_op.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Done running local_init_op.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Finished evaluation at 2019-12-08-22:01:36\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Finished evaluation at 2019-12-08-22:01:36\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saving dict for global step 5625: auc = 0.7589625, eval_accuracy = 0.859, f1_score = 0.9113765, false_negatives = 40.0, false_positives = 101.0, global_step = 5625, loss = 0.37621534, precision = 0.877724, recall = 0.9477124, true_negatives = 134.0, true_positives = 725.0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Saving dict for global step 5625: auc = 0.7589625, eval_accuracy = 0.859, f1_score = 0.9113765, false_negatives = 40.0, false_positives = 101.0, global_step = 5625, loss = 0.37621534, precision = 0.877724, recall = 0.9477124, true_negatives = 134.0, true_positives = 725.0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saving 'checkpoint_path' summary for global step 5625: model_out3/model.ckpt-5625\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Saving 'checkpoint_path' summary for global step 5625: model_out3/model.ckpt-5625\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["{'auc': 0.7589625,\n"," 'eval_accuracy': 0.859,\n"," 'f1_score': 0.9113765,\n"," 'false_negatives': 40.0,\n"," 'false_positives': 101.0,\n"," 'global_step': 5625,\n"," 'loss': 0.37621534,\n"," 'precision': 0.877724,\n"," 'recall': 0.9477124,\n"," 'true_negatives': 134.0,\n"," 'true_positives': 725.0}"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"markdown","metadata":{"id":"g8_mZ0LJDIw0","colab_type":"text"},"source":["#Evaluate on the test Set"]},{"cell_type":"code","metadata":{"id":"r1pbHprLphax","colab_type":"code","colab":{}},"source":["test_input_fn = input_fn_builder(\n","    dataframe=test,\n","    features=test_features,\n","    seq_length=MAX_SEQ_LENGTH,\n","    is_training=False,\n","    drop_remainder=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yVdAgow0przl","colab_type":"code","outputId":"8bd94d8d-8e9d-4306-c2bb-b1e444f4667e","executionInfo":{"status":"ok","timestamp":1575827748920,"user_tz":480,"elapsed":25620,"user":{"displayName":"Divya Sudhakar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBAEe8lSbL-gO8W2QGjfXvHbFVw-wl2B28bQD3_6_A=s64","userId":"13661723492181317645"}},"colab":{"base_uri":"https://localhost:8080/","height":598}},"source":["estimator.evaluate(input_fn=test_input_fn, steps=None)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Calling model_fn.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Calling model_fn.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Done calling model_fn.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Done calling model_fn.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Starting evaluation at 2019-12-08T17:55:28Z\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Starting evaluation at 2019-12-08T17:55:28Z\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Graph was finalized.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Graph was finalized.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from model_out2/model.ckpt-5625\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from model_out2/model.ckpt-5625\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Running local_init_op.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Running local_init_op.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Done running local_init_op.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Done running local_init_op.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Finished evaluation at 2019-12-08-17:55:48\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Finished evaluation at 2019-12-08-17:55:48\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saving dict for global step 5625: auc = 0.7443611, eval_accuracy = 0.8416816, f1_score = 0.89948887, false_negatives = 59.0, false_positives = 118.0, global_step = 5625, loss = 0.42407137, precision = 0.8703297, recall = 0.9306698, true_negatives = 149.0, true_positives = 792.0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Saving dict for global step 5625: auc = 0.7443611, eval_accuracy = 0.8416816, f1_score = 0.89948887, false_negatives = 59.0, false_positives = 118.0, global_step = 5625, loss = 0.42407137, precision = 0.8703297, recall = 0.9306698, true_negatives = 149.0, true_positives = 792.0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saving 'checkpoint_path' summary for global step 5625: model_out2/model.ckpt-5625\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Saving 'checkpoint_path' summary for global step 5625: model_out2/model.ckpt-5625\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["{'auc': 0.7443611,\n"," 'eval_accuracy': 0.8416816,\n"," 'f1_score': 0.89948887,\n"," 'false_negatives': 59.0,\n"," 'false_positives': 118.0,\n"," 'global_step': 5625,\n"," 'loss': 0.42407137,\n"," 'precision': 0.8703297,\n"," 'recall': 0.9306698,\n"," 'true_negatives': 149.0,\n"," 'true_positives': 792.0}"]},"metadata":{"tags":[]},"execution_count":51}]}]}